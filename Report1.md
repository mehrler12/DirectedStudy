# GPU Suitability of Adaptive Kalman Filters
`notes to myself for later are in code backticks`
## Introduction
Earth based telescopes often have to contend with conducting observations through atmospheric turbulence. An interesting binary star system may just look like a smudge with distortion from the atmosphere. Until the 90s the most feasible way to combat turbulence from the atmosphere was to put your telescope in space like Hubble or TESS. The sheer cost of putting your telescope in space is massive and there is only so much room in optimal orbits in space. Adaptive optics allows ground based telescopes to compensate for atmospheric distortion and achieve much higher resolutions. This lets us build massive telescopes on Earth that can observe the far reaches of space or potentially even image exoplanets. 

An adaptive optics system has several parts, a Wavefront Sensor (WFS), a control system, a Deformable Mirror (DM), and potentially one or more Laser Guide Stars (LGS). The WFS takes an image of a bright reference star (can be either a Natural Guide Star (NGS) or LGS) and sends data about the wavefront it received to the control system. The control system takes that data and compares it to how the NGS or LGS is supposed to look and figures out what shape the DM needs to be to make the image how it is supposed to look. The DM is a large mirror made up of a bunch of smaller pieces that can move around incredibly quickly, thus changing the shape of the whole mirror. Given the precision required for getting accurate images and fast changing nature of the atmosphere the control system needs to make its calculations incredibly quickly. 

The most time consuming step is taking the data coming from the WFS and reconstructing the waveform from it. A common method to do this is through an algorithm called Matrix Vector Multiply (MVM) which has O(n^2) time complexity with n being the size of the telescope. As we are building larger and larger telescopes (Thirty Meter Telescope, or the aptly named Extremely Large Telescope) this O(n^2) time complexity will no longer do. There are a large number of algorithms out there but one of particular interest is the Kalman Filter, it has O(n log n) time complexity and provides great accuracy in it's wavefront reconstructions. It also has the added benefit of being able to predict a step ahead fairly accurately. The Kalman filter is also well suited for parallelization on a Graphics Processing Unit (GPU).

A downside of using the Kalman filter is that you need good knowledge of the atmospheric conditions for it to work optimally, if the atmospheric conditions change the accuracy of the filter lowers significantly. An interesting proposed extension of the Kalman filter is the Adaptive Kalman Filter. This allows the algorithm to slightly adjust the matrices that represent the processes like wind speed or other noise sources to better compensate for changing conditions.

This extension to the Kalman Filter leads us to the research question that will be addressed by this report. **Does making the Kalman Filter adaptive add complexity that makes it less efficient to parallelize on a GPU?**. `Reading more on this this may be a real easy to answer question, if that ends up being the case i'll change it to "What approach to adaptive Kalman filtering is best suited for parallelization on a GPU?" and test out other approaches then the estimator one` Answering this question is important as it can tell us whether a wavefront reconstruction method based on Adaptive Kalman Filtering can be implemented on a GPU or whether another approach is required. It is also potentially useful in more than just large telescopes, adaptive optics is also used to image human eyes or to correct for atmospheric distortion in lasers shot into space for either satellite communication or removal of space debris. Adaptive Kalman filters are also used outside of adaptive optics in things like target tracking, robotics, or data fusion of sensor data.

To answer this question we will compare a GPU implementation of a basic KF to an AKF implementation using a estimator for noise. We'll compare the speed of both as well as their resepctive errors to assess their performance. As the AKF will inherently include more computations we will also look at various metrics of parallelization, including branch divergence, issue stalls, and occupancy, to assess how well the algorithm will scale to more powerful GPUs. 

## Background
### Kalman Filtering
Kalman Filtering is a process that allows us to accurately predict a value or set of values based of a series of inaccurate measurements. At a high level it works by predicting what the next state of the system will be and then combines it's prediction with the value it measured based on the error levels in both. 

The Kalman Filter works in 2 phases: predict and update. In the prediction phase it *predicts* the next value of the state as well as the new prediction error. For example if we were using the filter to track a car with GPS we would update our prediction based on the laws of kinematics. This step is represented by the *Predict* equation below, Xkp is the predicted state, A is a state transition matrix that moves the previous state to the next one, Xk-1 is the previous state, B and uk together are a control matrix which represents known forces that will affect the state (i.e acceleration), and Wk is an optional noise matrix. The final part of the prediction step is updating the process error to use later. This is shown in the *UpdateProcessError* equation below, Pkp is the new process error, Pk-1 is the previous process error and Qk is process noise covariance.

<a href="https://www.codecogs.com/eqnedit.php?latex=(Predict)&space;X_{k_{p}}=AX_{k-1}&plus;Bu_{k}&plus;w_{k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(Predict)&space;X_{k_{p}}=AX_{k-1}&plus;Bu_{k}&plus;w_{k}" title="(Predict) X_{k_{p}}=AX_{k-1}+Bu_{k}+w_{k}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=(Process&space;Error&space;Update)&space;P_{k_{p}}=AP_{k-1}A^{T}&space;&plus;Q_{k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(Process&space;Error&space;Update)&space;P_{k_{p}}=AP_{k-1}A^{T}&space;&plus;Q_{k}" title="(Process Error Update) P_{k_{p}}=AP_{k-1}A^{T} +Q_{k}" /></a>

The update stage takes the measured value and *combines* it with the prediction. It does this by first calculating the Kalman Gain which is a ratio of how much it should trust the measurement compared to how much it should trust the prediction. This ratio is based off the error in the measurement and the prediction noise. This is shown by the *KalmanGain* Equation below, H is a transformation matrix and R is the measurement noise covariance. We then multiply the ratio by the diffrence between the measurement and the prediction and then add that back to the prediction to get our new state. This is shown in the *UpdatePrediction* equation below, Y is the measured values. We then update the process error again. This is shown in the *UpdateProcessError* equation below, I represents the identity matrix.

<a href="https://www.codecogs.com/eqnedit.php?latex=(Kalman&space;Gain)&space;K=\frac{P_{k_{p}}H^{T}}{HP_{k_{p}}H^T&plus;R}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(Kalman&space;Gain)&space;K=\frac{P_{k_{p}}H^{T}}{HP_{k_{p}}H^T&plus;R}" title="(Kalman Gain) K=\frac{P_{k_{p}}H^{T}}{HP_{k_{p}}H^T+R}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=(UpdatePrediction)&space;X_k=X_{k_p}&plus;K[Y-HX_k]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(UpdatePrediction)&space;X_k=X_{k_p}&plus;K[Y-HX_k]" title="(UpdatePrediction) X_k=X_{k_p}+K[Y-HX_k]" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=(UpdateProcessError)&space;P_k=(I-KH)P_{k_p}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(UpdateProcessError)&space;P_k=(I-KH)P_{k_p}" title="(UpdateProcessError) P_k=(I-KH)P_{k_p}" /></a>

These steps are then repeated for every time step to filter the incoming measurements and output a more accurate measurement.

### Adaptive Kalman Filtering
For the Kalman Filter to work effectively we need to know the noise matrices Q and R from the equations above. Most examples of the Kalman Filter calculate these before the filter is running from prior knowledge of the system. However in some cases we don't know these noise matrices beforehand or they may change as time goes on (i.e changes in wind speed or sharp turns in a car). Making the filter adaptive allows us to estimate Q and R as part of each step, this ultimately makes the filter more accurate as it can adapt to new conditions. There are a few common ways to do this which will be surveyed in the related woks section below.

Introducing this adaptive step into the filter process will potentially affect parallelization depending on how it's implemented. Constantly changing Q and R will potentially cause a data dependcy on them as the GPU cannot work as far ahead until it knows the value of Q and R. Additionally the process of estimating Q and R may not be parallelizable which would make the whole algorithm subject to Amahdl's law.

## Related Work
### Kalman Filtering in Adaptive Optics
Overview of papers on kalman filtering of contributions from papers on KF in AO, will include most papers from the bib
### Adaptive Kalman Filtering
Overview of papers on making the kalman filter adaptive, outline various methods people use. (May not be nessecary as much will be covered in the background section)
### Kalman Filters on GPUs
Basic Kalman Filters have been shown to be quite effective when running on a GPU. Huang et. al. present a GPU implementation of the Kalman Filter with a maximum observed speedup of 7398x [1][Huang2011]. Campora and Awile present a more general SIMD approach for use at the LHC in CERN [2][Campora2018].

[Hung2011]: https://ieeexplore.ieee.org/document/6121397 "M.-Y. Huang, S.-C. Wei, B. Huang, and Y.-L. Chang, “Accelerating the Kalman Filter on a GPU,” in 2011 IEEE 17th International Conference on Parallel and Distributed Systems, 2011, pp. 1016–1020, doi: 10.1109/ICPADS.2011.153."

[Campora2018]: https://onlinelibrary.wiley.com/doi/full/10.1002/cpe.4483 "D. H. Campora Perez and O. Awile, “An efficient low-rank Kalman filter for modern SIMD architectures,” Concurrency and Computation: Practice and Experience, vol. 30, no. 23, p. e4483, 2018."

